{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a405be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import requests\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "DATALAKE_ROOT_FOLDER = HOME + \"/datalake/\"\n",
    "\n",
    "\n",
    "def fetch_data_from_imdb1(**kwargs):\n",
    "   current_day = date.today().strftime(\"%Y%m%d\")\n",
    "   TARGET_PATH = DATALAKE_ROOT_FOLDER + \"raw/imdb/MovieRating/\" + current_day + \"/\"\n",
    "   if not os.path.exists(TARGET_PATH):\n",
    "       os.makedirs(TARGET_PATH)\n",
    "\n",
    "   url = 'https://datasets.imdbws.com/title.ratings.tsv.gz'\n",
    "   r = requests.get(url, allow_redirects=True)\n",
    "   open(TARGET_PATH + 'title.ratings.tsv.gz', 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed32cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data_from_imdb1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14550449",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.path.expanduser('~')\n",
    "DATALAKE_ROOT_FOLDER = HOME + \"/datalake/\"\n",
    "\n",
    "def fetch_data_from_imdb2(url, data_entity_name, **kwargs):\n",
    "    current_day = date.today().strftime(\"%Y%m%d\")\n",
    "    TARGET_PATH = DATALAKE_ROOT_FOLDER + \"raw/imdb/\" + data_entity_name + \"/\" + current_day + \"/\"\n",
    "    if not os.path.exists(TARGET_PATH):\n",
    "        os.makedirs(TARGET_PATH)\n",
    "        \n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    file_path = os.path.join(TARGET_PATH, file_name)\n",
    "    \n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(file_path, 'wb').write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f4c5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_data_from_imdb2('https://datasets.imdbws.com/title.ratings.tsv.gz', 'MovieRating')\n",
    "fetch_data_from_imdb2('https://datasets.imdbws.com/name.basics.tsv.gz', 'MovieName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e819f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "DATALAKE_ROOT_FOLDER = HOME + \"/datalake/\"\n",
    "\n",
    "\n",
    "def convert_raw_to_formatted(file_name, current_day):\n",
    "   RATING_PATH = DATALAKE_ROOT_FOLDER + \"raw/imdb/MovieRating/\" + current_day + \"/\" + file_name\n",
    "   FORMATTED_RATING_FOLDER = DATALAKE_ROOT_FOLDER + \"formatted/imdb/MovieRating/\" + current_day + \"/\"\n",
    "   if not os.path.exists(FORMATTED_RATING_FOLDER):\n",
    "       os.makedirs(FORMATTED_RATING_FOLDER)\n",
    "   df = pd.read_csv(RATING_PATH, sep='\\t')\n",
    "   parquet_file_name = file_name.replace(\".tsv.gz\", \".snappy.parquet\")\n",
    "   df.to_parquet(FORMATTED_RATING_FOLDER + parquet_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e9b3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file name and current day\n",
    "file_name = \"title.ratings.tsv.gz\"\n",
    "current_day = \"20230606\"\n",
    "\n",
    "# Call the function\n",
    "convert_raw_to_formatted(file_name, current_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00e50f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "HOME = os.path.expanduser('~')\n",
    "DATALAKE_ROOT_FOLDER = HOME + \"/datalake/\"\n",
    "\n",
    "\n",
    "def combine_data(current_day):\n",
    "   RATING_PATH = DATALAKE_ROOT_FOLDER + \"formatted/imdb/MovieRating/\" + current_day + \"/\"\n",
    "   USAGE_OUTPUT_FOLDER_STATS = DATALAKE_ROOT_FOLDER + \"usage/movieAnalysis/MovieStatistics/\" + current_day + \"/\"\n",
    "   USAGE_OUTPUT_FOLDER_BEST = DATALAKE_ROOT_FOLDER + \"usage/movieAnalysis/MovieTop10/\" + current_day + \"/\"\n",
    "   if not os.path.exists(USAGE_OUTPUT_FOLDER_STATS):\n",
    "       os.makedirs(USAGE_OUTPUT_FOLDER_STATS)\n",
    "   if not os.path.exists(USAGE_OUTPUT_FOLDER_BEST):\n",
    "       os.makedirs(USAGE_OUTPUT_FOLDER_BEST)\n",
    "\n",
    "   from pyspark import SparkContext\n",
    "\n",
    "   sc = SparkContext(appName=\"CombineData\")\n",
    "   sqlContext = SQLContext(sc)\n",
    "   df_ratings = sqlContext.read.parquet(RATING_PATH)\n",
    "   df_ratings.registerTempTable(\"ratings\")\n",
    "\n",
    "   # Check content of the DataFrame df_ratings:\n",
    "   print(df_ratings.show())\n",
    "\n",
    "   stats_df = sqlContext.sql(\"SELECT AVG(averageRating) AS avg_rating,\"\n",
    "                             \"       MAX(averageRating) AS max_rating,\"\n",
    "                             \"       MIN(averageRating) AS min_rating,\"\n",
    "                             \"       COUNT(averageRating) AS count_rating\"\n",
    "                             \"    FROM ratings LIMIT 10\")\n",
    "   top10_df = sqlContext.sql(\"SELECT tconst, averageRating\"\n",
    "                             \"    FROM ratings\"\n",
    "                             \"    WHERE numVotes > 50000 \"\n",
    "                             \"    ORDER BY averageRating DESC\"\n",
    "                             \"    LIMIT 10\")\n",
    "\n",
    "   # Check content of the DataFrame stats_df and save it:\n",
    "   print(stats_df.show())\n",
    "   stats_df.write.save(USAGE_OUTPUT_FOLDER_STATS + \"res.snappy.parquet\", mode=\"overwrite\")\n",
    "\n",
    "   # Check content of the DataFrame top10_df  and save it:\n",
    "   print(top10_df.show())\n",
    "   stats_df.write.save(USAGE_OUTPUT_FOLDER_BEST + \"res.snappy.parquet\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f6a460",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=CombineData, master=local[*]) created by __init__ at C:\\Users\\qcxl9\\AppData\\Local\\Temp\\ipykernel_44300\\3611609488.py:19 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m current_day \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m20230606\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcombine_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_day\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m, in \u001b[0;36mcombine_data\u001b[1;34m(current_day)\u001b[0m\n\u001b[0;32m     15\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(USAGE_OUTPUT_FOLDER_BEST)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m---> 19\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mappName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCombineData\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n\u001b[0;32m     21\u001b[0m df_ratings \u001b[38;5;241m=\u001b[39m sqlContext\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(RATING_PATH)\n",
      "File \u001b[1;32mD:\\software\\Conda\\lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32mD:\\software\\Conda\\lib\\site-packages\\pyspark\\context.py:445\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    450\u001b[0m             currentAppName,\n\u001b[0;32m    451\u001b[0m             currentMaster,\n\u001b[0;32m    452\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    453\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    454\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    455\u001b[0m         )\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=CombineData, master=local[*]) created by __init__ at C:\\Users\\qcxl9\\AppData\\Local\\Temp\\ipykernel_44300\\3611609488.py:19 "
     ]
    }
   ],
   "source": [
    "# Specify the current day\n",
    "current_day = \"20230606\"\n",
    "\n",
    "# Call the function\n",
    "combine_data(current_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0534b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49ceb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
